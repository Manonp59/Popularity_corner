# Use the official Apache Airflow image as a base image
FROM apache/airflow:2.2.0

# Copy requirements and install them as root
COPY requirements.txt /tmp/requirements.txt

# Copy dependecies and install them as root
COPY requirements.sh /tmp/requirements.sh
RUN sh /tmp/requirements.sh

# Copy the custom configuration file into the container
COPY airflow.cfg /usr/local/airflow/airflow.cfg

# Set the AIRFLOW_HOME variable
ENV AIRFLOW_HOME=/usr/local/airflow

# Change to root user to change permissions and perform operations requiring higher privileges
USER root

# Copy drivers and install them as root
COPY odbc.sh /tmp/odbc.sh
RUN sh /tmp/odbc.sh

# Change permissions of AIRFLOW_HOME directory
RUN mkdir -p $AIRFLOW_HOME && chown -R airflow: $AIRFLOW_HOME

# Copy the start-airflow.sh script into the image and make it executable
COPY entrypoint.sh /usr/local/bin/entrypoint.sh
RUN chmod +x /usr/local/bin/entrypoint.sh

# Switch back to airflow user for the remaining operations
USER airflow

# Copy your dags (spider) to the image
COPY --chown=airflow:airflow /dags /usr/local/airflow/dags

# Copy the Scrapy project
COPY incoming_movies_spider/ /opt/airflow/incoming_movies_spider
COPY last_week_spider/ /opt/airflow/last_week_spider

# Set Airflow to use the SequentialExecutor
ENV AIRFLOW__CORE__EXECUTOR=SequentialExecutor

# Initialize the database
RUN airflow db init

# Create a user for accessing the web interface (optional)
RUN airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname Admin \
    --role Admin \
    --email admin@example.com

# Expose the web server's port
EXPOSE 8080

# Start the web server and scheduler
ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]